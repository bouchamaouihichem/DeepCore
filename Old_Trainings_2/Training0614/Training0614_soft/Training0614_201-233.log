2022-06-19 23:39:32.703847: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-19 23:39:33.590020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4614 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0
2022-06-19 23:40:51.795480: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:206: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
2022-06-19 23:40:54.088208: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
jetUps= (None, 30, 30, 2)
NNinputs= (None, 30, 30, 4)
ComplInput= (None, 30, 30, 6)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 2)            0           input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
reshape (Reshape)               (None, 1, 1, 2)      0           concatenate[0][0]                
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 30, 30, 4)]  0                                            
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 30, 30, 2)    0           reshape[0][0]                    
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 30, 6)    0           input_3[0][0]                    
                                                                 up_sampling2d[0][0]              
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 30, 30, 50)   14750       concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 30, 30, 20)   25020       conv2d[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 30, 30, 20)   10020       conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 30, 18)   9018        conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 30, 18)   2934        conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 30, 30, 18)   2934        conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 30, 30, 12)   1956        conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 30, 30, 18)   2934        conv2d_5[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 30, 30, 9)    981         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 30, 18)   2934        conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 30, 30, 7)    574         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 30, 18)   2934        conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 30, 30, 6)    384         conv2d_11[0][0]                  
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 30, 30, 3, 6) 0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
reshape_2 (Reshape)             (None, 30, 30, 3, 2) 0           conv2d_12[0][0]                  
==================================================================================================
Total params: 77,373
Trainable params: 77,373
Non-trainable params: 0
__________________________________________________________________________________________________
number of  file= 273
number of file validation= 68
total number of events = 8341541
total number of events validation= 2079273
Number of Steps= 130336.578125
training: start
Epoch 201/233
6516/6516 - 581s - loss: 2.3200 - reshape_1_loss: 0.4342 - reshape_2_loss: 1.8858 - val_loss: 1.2347 - val_reshape_1_loss: 0.4349 - val_reshape_2_loss: 0.7998

Epoch 00201: saving model to weights.201-1.23.hdf5
Epoch 202/233
6516/6516 - 566s - loss: 1.2309 - reshape_1_loss: 0.4299 - reshape_2_loss: 0.8010 - val_loss: 1.2271 - val_reshape_1_loss: 0.4315 - val_reshape_2_loss: 0.7956

Epoch 00202: saving model to weights.202-1.23.hdf5
Epoch 203/233
6516/6516 - 566s - loss: 1.5451 - reshape_1_loss: 0.4277 - reshape_2_loss: 1.1174 - val_loss: 1.1995 - val_reshape_1_loss: 0.4254 - val_reshape_2_loss: 0.7741

Epoch 00203: saving model to weights.203-1.20.hdf5
Epoch 204/233
6516/6516 - 564s - loss: 1.2006 - reshape_1_loss: 0.4271 - reshape_2_loss: 0.7735 - val_loss: 1.1877 - val_reshape_1_loss: 0.4230 - val_reshape_2_loss: 0.7646

Epoch 00204: saving model to weights.204-1.19.hdf5
Epoch 205/233
6516/6516 - 565s - loss: 2.9426 - reshape_1_loss: 0.4257 - reshape_2_loss: 2.5169 - val_loss: 1.2035 - val_reshape_1_loss: 0.4242 - val_reshape_2_loss: 0.7793

Epoch 00205: saving model to weights.205-1.20.hdf5
Epoch 206/233
6516/6516 - 564s - loss: 1.2006 - reshape_1_loss: 0.4260 - reshape_2_loss: 0.7746 - val_loss: 1.1884 - val_reshape_1_loss: 0.4256 - val_reshape_2_loss: 0.7628

Epoch 00206: saving model to weights.206-1.19.hdf5
Epoch 207/233
6516/6516 - 564s - loss: 1.1864 - reshape_1_loss: 0.4235 - reshape_2_loss: 0.7629 - val_loss: 1.1811 - val_reshape_1_loss: 0.4231 - val_reshape_2_loss: 0.7580

Epoch 00207: saving model to weights.207-1.18.hdf5
Epoch 208/233
6516/6516 - 562s - loss: 1.1710 - reshape_1_loss: 0.4207 - reshape_2_loss: 0.7502 - val_loss: 1.1509 - val_reshape_1_loss: 0.4152 - val_reshape_2_loss: 0.7357

Epoch 00208: saving model to weights.208-1.15.hdf5
Epoch 209/233
6516/6516 - 565s - loss: 17.4625 - reshape_1_loss: 0.4191 - reshape_2_loss: 17.0434 - val_loss: 1.3082 - val_reshape_1_loss: 0.4260 - val_reshape_2_loss: 0.8822

Epoch 00209: saving model to weights.209-1.31.hdf5
Epoch 210/233
6516/6516 - 566s - loss: 1.3039 - reshape_1_loss: 0.4205 - reshape_2_loss: 0.8834 - val_loss: 1.3016 - val_reshape_1_loss: 0.4200 - val_reshape_2_loss: 0.8816

Epoch 00210: saving model to weights.210-1.30.hdf5
Epoch 211/233
6516/6516 - 585s - loss: 1.2633 - reshape_1_loss: 0.4223 - reshape_2_loss: 0.8410 - val_loss: 1.2102 - val_reshape_1_loss: 0.4275 - val_reshape_2_loss: 0.7827

Epoch 00211: saving model to weights.211-1.21.hdf5
Epoch 212/233
6516/6516 - 566s - loss: 1.1663 - reshape_1_loss: 0.4189 - reshape_2_loss: 0.7474 - val_loss: 1.1462 - val_reshape_1_loss: 0.4140 - val_reshape_2_loss: 0.7323

Epoch 00212: saving model to weights.212-1.15.hdf5
Epoch 213/233
6516/6516 - 567s - loss: 5.2285 - reshape_1_loss: 0.4131 - reshape_2_loss: 4.8154 - val_loss: 1.1545 - val_reshape_1_loss: 0.4170 - val_reshape_2_loss: 0.7375

Epoch 00213: saving model to weights.213-1.15.hdf5
Epoch 214/233
6516/6516 - 571s - loss: 1.1437 - reshape_1_loss: 0.4108 - reshape_2_loss: 0.7328 - val_loss: 3.6230 - val_reshape_1_loss: 0.4075 - val_reshape_2_loss: 3.2155

Epoch 00214: saving model to weights.214-3.62.hdf5
Epoch 215/233
6516/6516 - 562s - loss: 1.7199 - reshape_1_loss: 0.4270 - reshape_2_loss: 1.2929 - val_loss: 1.2169 - val_reshape_1_loss: 0.4364 - val_reshape_2_loss: 0.7805

Epoch 00215: saving model to weights.215-1.22.hdf5
Epoch 216/233
6516/6516 - 566s - loss: 1.4133 - reshape_1_loss: 0.4361 - reshape_2_loss: 0.9772 - val_loss: 1.1819 - val_reshape_1_loss: 0.4322 - val_reshape_2_loss: 0.7497

Epoch 00216: saving model to weights.216-1.18.hdf5
Epoch 217/233
6516/6516 - 562s - loss: 2.2172 - reshape_1_loss: 0.4213 - reshape_2_loss: 1.7959 - val_loss: 2.1373 - val_reshape_1_loss: 0.4160 - val_reshape_2_loss: 1.7214

Epoch 00217: saving model to weights.217-2.14.hdf5
Epoch 218/233
6516/6516 - 564s - loss: 1.1546 - reshape_1_loss: 0.4158 - reshape_2_loss: 0.7388 - val_loss: 1.1498 - val_reshape_1_loss: 0.4117 - val_reshape_2_loss: 0.7381

Epoch 00218: saving model to weights.218-1.15.hdf5
Epoch 219/233
6516/6516 - 567s - loss: 1.1339 - reshape_1_loss: 0.4123 - reshape_2_loss: 0.7216 - val_loss: 1.1140 - val_reshape_1_loss: 0.4120 - val_reshape_2_loss: 0.7021

Epoch 00219: saving model to weights.219-1.11.hdf5
Epoch 220/233
6516/6516 - 564s - loss: 1.1048 - reshape_1_loss: 0.4100 - reshape_2_loss: 0.6948 - val_loss: 1.0998 - val_reshape_1_loss: 0.4113 - val_reshape_2_loss: 0.6884

Epoch 00220: saving model to weights.220-1.10.hdf5
Epoch 221/233
6516/6516 - 565s - loss: 1.9721 - reshape_1_loss: 0.4088 - reshape_2_loss: 1.5633 - val_loss: 1.1002 - val_reshape_1_loss: 0.4099 - val_reshape_2_loss: 0.6904

Epoch 00221: saving model to weights.221-1.10.hdf5
Epoch 222/233
6516/6516 - 568s - loss: 1.0987 - reshape_1_loss: 0.4052 - reshape_2_loss: 0.6935 - val_loss: 1.0988 - val_reshape_1_loss: 0.4056 - val_reshape_2_loss: 0.6933

Epoch 00222: saving model to weights.222-1.10.hdf5
Epoch 223/233
6516/6516 - 572s - loss: 1.0880 - reshape_1_loss: 0.4040 - reshape_2_loss: 0.6840 - val_loss: 1.0713 - val_reshape_1_loss: 0.4019 - val_reshape_2_loss: 0.6694

Epoch 00223: saving model to weights.223-1.07.hdf5
Epoch 224/233
6516/6516 - 560s - loss: 1.4020 - reshape_1_loss: 0.4038 - reshape_2_loss: 0.9982 - val_loss: 1.0675 - val_reshape_1_loss: 0.4018 - val_reshape_2_loss: 0.6657

Epoch 00224: saving model to weights.224-1.07.hdf5
Epoch 225/233
6516/6516 - 558s - loss: 1.6877 - reshape_1_loss: 0.4038 - reshape_2_loss: 1.2839 - val_loss: 1.0693 - val_reshape_1_loss: 0.4009 - val_reshape_2_loss: 0.6684

Epoch 00225: saving model to weights.225-1.07.hdf5
Epoch 226/233
6516/6516 - 554s - loss: 1.0716 - reshape_1_loss: 0.4035 - reshape_2_loss: 0.6681 - val_loss: 1.0677 - val_reshape_1_loss: 0.4055 - val_reshape_2_loss: 0.6622

Epoch 00226: saving model to weights.226-1.07.hdf5
Epoch 227/233
6516/6516 - 553s - loss: 1.0674 - reshape_1_loss: 0.4039 - reshape_2_loss: 0.6634 - val_loss: 1.0583 - val_reshape_1_loss: 0.4007 - val_reshape_2_loss: 0.6577

Epoch 00227: saving model to weights.227-1.06.hdf5
Epoch 228/233
6516/6516 - 554s - loss: 1.0542 - reshape_1_loss: 0.3998 - reshape_2_loss: 0.6543 - val_loss: 1.0402 - val_reshape_1_loss: 0.3962 - val_reshape_2_loss: 0.6439

Epoch 00228: saving model to weights.228-1.04.hdf5
Epoch 229/233
6516/6516 - 548s - loss: 1.5696 - reshape_1_loss: 0.3997 - reshape_2_loss: 1.1699 - val_loss: 1.0535 - val_reshape_1_loss: 0.4010 - val_reshape_2_loss: 0.6525

Epoch 00229: saving model to weights.229-1.05.hdf5
Epoch 230/233
6516/6516 - 696s - loss: 14.8575 - reshape_1_loss: 0.4002 - reshape_2_loss: 14.4573 - val_loss: 1.0994 - val_reshape_1_loss: 0.4011 - val_reshape_2_loss: 0.6983

Epoch 00230: saving model to weights.230-1.10.hdf5
Epoch 231/233
6516/6516 - 674s - loss: 1.0936 - reshape_1_loss: 0.3989 - reshape_2_loss: 0.6947 - val_loss: 1.0929 - val_reshape_1_loss: 0.4014 - val_reshape_2_loss: 0.6915

Epoch 00231: saving model to weights.231-1.09.hdf5
Epoch 232/233
6516/6516 - 631s - loss: 1.0618 - reshape_1_loss: 0.3969 - reshape_2_loss: 0.6650 - val_loss: 1.0422 - val_reshape_1_loss: 0.3972 - val_reshape_2_loss: 0.6451

Epoch 00232: saving model to weights.232-1.04.hdf5
Epoch 233/233
6516/6516 - 666s - loss: 4.9185 - reshape_1_loss: 0.3950 - reshape_2_loss: 4.5235 - val_loss: 1.0466 - val_reshape_1_loss: 0.3965 - val_reshape_2_loss: 0.6500

Epoch 00233: saving model to weights.233-1.05.hdf5
done running; now save
training: completed
